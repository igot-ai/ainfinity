defaults:
  - _self_
  - dataset: orca_chat
  - model: qwen3

seed: 0

training_arguments:
  optim: paged_adamw_32bit
  bf16: true
  fp16: false
  do_train: true
  do_eval: true
  do_predict: true
  output_dir: "./finetuning-checkpoints"
  num_train_epochs: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 1
  eval_strategy: "epoch"
  save_strategy: "epoch"
  learning_rate: 5e-05
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.01
  weight_decay: 0.01
  logging_steps: 10
  tf32: true
  gradient_checkpointing: false
  disable_tqdm: true
  report_to: "wandb"
  logging_dir: "./logs"
  torch_compile: false
  save_total_limit: 3
  overwrite_output_dir: true
  logging_strategy: steps
  seed: 0
  push_to_hub: true
  use_liger_kernel: true
